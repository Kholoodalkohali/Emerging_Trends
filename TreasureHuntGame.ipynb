{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4843b7aa-d1be-402a-9d81-c8121fc3f711",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TreasureHuntGame.ipynb Implementation\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Required Imports\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Flatten\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# TreasureHuntGame.ipynb Implementation\n",
    "# Required Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from GameExperience import GameExperience\n",
    "from TreasureMaze import TreasureMaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851c7207-7cfb-4660-89ff-64fa94b55265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epsilon = 0.1  # Exploration factor\n",
    "learning_rate = 0.001\n",
    "gamma = 0.95  # Discount factor\n",
    "max_memory = 500\n",
    "data_size = 50\n",
    "epochs = 1000\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55701ed6-72db-40ff-ade8-a6ebcafb1589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Maze\n",
    "maze = np.array([\n",
    "    [1, 1, 1, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [1, 1, 1, 1],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "pirate_initial_position = (0, 0)\n",
    "maze_env = TreasureMaze(maze, pirate_initial_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e3d59e-c522-4785-a07b-f06a498b6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "def build_model(input_shape, output_shape):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=input_shape),\n",
    "        Dense(24, activation='relu'),\n",
    "        Dense(24, activation='relu'),\n",
    "        Dense(output_shape, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8183c1-b0ac-4330-ba6f-7b3c9d27857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model\n",
    "input_shape = maze_env.observe().shape\n",
    "output_shape = 4  # Number of possible actions\n",
    "model = build_model(input_shape, output_shape)\n",
    "experience = GameExperience(model, max_memory=max_memory, discount=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff1aee5-ff45-4992-83b6-6cdda7ff00dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    maze_env.reset(pirate_initial_position)\n",
    "    game_status = 'not_over'\n",
    "    total_loss = 0\n",
    "    while game_status == 'not_over':\n",
    "        # Exploration vs Exploitation\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(maze_env.valid_actions())\n",
    "        else:\n",
    "            q_values = experience.predict(maze_env.observe())\n",
    "            action = np.argmax(q_values)\n",
    "\n",
    "        # Execute action and observe\n",
    "        envstate, reward, game_status = maze_env.act(action)\n",
    "        envstate_next = maze_env.observe()\n",
    "\n",
    "        # Store experience\n",
    "        experience.remember([envstate, action, reward, envstate_next, game_status == 'lose'])\n",
    "\n",
    "        # Training\n",
    "        inputs, targets = experience.get_data(data_size)\n",
    "        loss = model.train_on_batch(inputs, targets)\n",
    "        total_loss += loss\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
